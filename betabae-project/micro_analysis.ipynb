{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MicroBetaBae: Sutton's Bitter Lesson Implementation\n",
        "\n",
        "This notebook analyzes the MicroBetaBae implementation that follows Sutton's Bitter Lesson principles:\n",
        "\n",
        "## Key Design Principles:\n",
        "\n",
        "### 1. **Sutton's Bitter Lesson**: Computation > Knowledge\n",
        "- Use search-based action selection instead of hand-crafted heuristics\n",
        "- Let the model learn everything through computation\n",
        "- Minimal prior knowledge, maximum learning capacity\n",
        "\n",
        "### 2. **DeepSeek R1**: Reasoning through Search and Reflection\n",
        "- MCTS-like search for action selection\n",
        "- Reflection learning from past mistakes\n",
        "- Replay buffer for experience replay\n",
        "\n",
        "### 3. **Micrograd**: Minimal Autograd\n",
        "- Custom autograd implementation (no PyTorch dependency)\n",
        "- Efficient memory usage for scaling past 10,000 episodes\n",
        "- Pure Python implementation for maximum compatibility\n",
        "\n",
        "### 4. **Scalability**: Efficient Memory Management\n",
        "- Fixed-size buffers to prevent memory leaks\n",
        "- Efficient logging and checkpointing\n",
        "- Works on resource-constrained systems\n",
        "\n",
        "## What We're Observing:\n",
        "- **Search Quality**: How does MCTS-like search improve decisions?\n",
        "- **Reflection Learning**: Does learning from mistakes accelerate convergence?\n",
        "- **Attention Patterns**: How does attention evolve with search and reflection?\n",
        "- **Scalability**: Can it maintain performance past 10,000 episodes?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check if MicroBetaBae training has produced results\n",
        "output_dir = Path('micro_outputs')\n",
        "log_dir = output_dir / 'logs'\n",
        "\n",
        "print(\"MicroBetaBae Analysis\")\n",
        "print(\"====================\")\n",
        "\n",
        "if output_dir.exists():\n",
        "    stats_files = sorted(output_dir.glob('stats_ep_*.json'))\n",
        "    print(f\"Found {len(stats_files)} statistics files\")\n",
        "    \n",
        "    if stats_files:\n",
        "        # Load latest stats\n",
        "        with open(stats_files[-1], 'r') as f:\n",
        "            stats = json.load(f)\n",
        "        \n",
        "        print(f\"Training episodes: {len(stats['rewards'])}\")\n",
        "        print(f\"Elapsed time: {stats.get('elapsed_time', 0):.2f} seconds\")\n",
        "        print(f\"Episodes per second: {len(stats['rewards']) / max(stats.get('elapsed_time', 1), 1):.2f}\")\n",
        "        \n",
        "        # Show recent performance\n",
        "        recent_rewards = stats['rewards'][-100:] if len(stats['rewards']) >= 100 else stats['rewards']\n",
        "        recent_lengths = stats['lengths'][-100:] if len(stats['lengths']) >= 100 else stats['lengths']\n",
        "        \n",
        "        print(f\"Recent average reward: {np.mean(recent_rewards):.2f}\")\n",
        "        print(f\"Recent average length: {np.mean(recent_lengths):.1f} steps\")\n",
        "        \n",
        "        if len(stats['rewards']) >= 200:\n",
        "            early_reward = np.mean(stats['rewards'][:100])\n",
        "            late_reward = np.mean(stats['rewards'][-100:])\n",
        "            improvement = late_reward - early_reward\n",
        "            print(f\"Learning improvement: {improvement:.2f} ({improvement/early_reward*100:.1f}%)\")\n",
        "    \n",
        "    # Check for attention data\n",
        "    if log_dir.exists():\n",
        "        attention_files = sorted(log_dir.glob('attention_ep_*.npy'))\n",
        "        hidden_files = sorted(log_dir.glob('hidden_ep_*.npy'))\n",
        "        print(f\"Found {len(attention_files)} attention files\")\n",
        "        print(f\"Found {len(hidden_files)} hidden state files\")\n",
        "else:\n",
        "    print(\"MicroBetaBae training not found. Training may still be in progress...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze MicroBetaBae Learning Dynamics\n",
        "def analyze_micro_learning():\n",
        "    \"\"\"Comprehensive analysis of MicroBetaBae learning\"\"\"\n",
        "    \n",
        "    if not output_dir.exists():\n",
        "        print(\"No training data available yet!\")\n",
        "        return\n",
        "    \n",
        "    stats_files = sorted(output_dir.glob('stats_ep_*.json'))\n",
        "    if not stats_files:\n",
        "        print(\"No statistics files found!\")\n",
        "        return\n",
        "    \n",
        "    # Load all statistics\n",
        "    with open(stats_files[-1], 'r') as f:\n",
        "        stats = json.load(f)\n",
        "    \n",
        "    rewards = np.array(stats['rewards'])\n",
        "    lengths = np.array(stats['lengths'])\n",
        "    losses = np.array(stats['losses'])\n",
        "    \n",
        "    # Create comprehensive analysis\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # 1. Episode rewards over time\n",
        "    axes[0, 0].plot(rewards, alpha=0.7, linewidth=0.5)\n",
        "    axes[0, 0].set_title('Episode Rewards Evolution')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Episode lengths over time\n",
        "    axes[0, 1].plot(lengths, alpha=0.7, linewidth=0.5, color='orange')\n",
        "    axes[0, 1].set_title('Episode Lengths Evolution')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Steps')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Episode losses over time\n",
        "    axes[0, 2].plot(losses, alpha=0.7, linewidth=0.5, color='green')\n",
        "    axes[0, 2].set_title('Episode Losses Evolution')\n",
        "    axes[0, 2].set_xlabel('Episode')\n",
        "    axes[0, 2].set_ylabel('Loss')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Rolling averages\n",
        "    window = 50\n",
        "    if len(rewards) >= window:\n",
        "        rolling_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        rolling_lengths = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
        "        \n",
        "        axes[1, 0].plot(rolling_rewards, label=f'Reward (window={window})', linewidth=2)\n",
        "        axes[1, 0].plot(rolling_lengths, label=f'Length (window={window})', linewidth=2)\n",
        "        axes[1, 0].set_title('Rolling Averages')\n",
        "        axes[1, 0].set_xlabel('Episode')\n",
        "        axes[1, 0].set_ylabel('Value')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Learning phases analysis\n",
        "    if len(rewards) >= 300:\n",
        "        # Divide into phases\n",
        "        phase_size = len(rewards) // 3\n",
        "        phases = ['Early', 'Middle', 'Late']\n",
        "        phase_rewards = [rewards[:phase_size], rewards[phase_size:2*phase_size], rewards[2*phase_size:]]\n",
        "        \n",
        "        phase_means = [np.mean(phase) for phase in phase_rewards]\n",
        "        phase_stds = [np.std(phase) for phase in phase_rewards]\n",
        "        \n",
        "        x_pos = np.arange(len(phases))\n",
        "        axes[1, 1].bar(x_pos, phase_means, yerr=phase_stds, capsize=5, alpha=0.7)\n",
        "        axes[1, 1].set_title('Learning Phases Comparison')\n",
        "        axes[1, 1].set_xlabel('Phase')\n",
        "        axes[1, 1].set_ylabel('Average Reward')\n",
        "        axes[1, 1].set_xticks(x_pos)\n",
        "        axes[1, 1].set_xticklabels(phases)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Performance distribution\n",
        "    axes[1, 2].hist(rewards, bins=30, alpha=0.7, color='purple')\n",
        "    axes[1, 2].set_title('Reward Distribution')\n",
        "    axes[1, 2].set_xlabel('Reward')\n",
        "    axes[1, 2].set_ylabel('Frequency')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print key insights\n",
        "    print(f\"\\nMicroBetaBae Learning Analysis:\")\n",
        "    print(f\"Total episodes: {len(rewards)}\")\n",
        "    print(f\"Final average reward: {np.mean(rewards[-100:]):.2f}\")\n",
        "    print(f\"Final average length: {np.mean(lengths[-100:]):.1f}\")\n",
        "    print(f\"Final average loss: {np.mean(losses[-100:]):.4f}\")\n",
        "    \n",
        "    if len(rewards) >= 200:\n",
        "        early_perf = np.mean(rewards[:100])\n",
        "        late_perf = np.mean(rewards[-100:])\n",
        "        improvement = late_perf - early_perf\n",
        "        print(f\"Learning improvement: {improvement:.2f} ({improvement/early_perf*100:.1f}%)\")\n",
        "    \n",
        "    # Check for convergence\n",
        "    if len(rewards) >= 500:\n",
        "        recent_std = np.std(rewards[-100:])\n",
        "        print(f\"Recent performance stability (std): {recent_std:.2f}\")\n",
        "        if recent_std < 10:\n",
        "            print(\"✓ Model appears to have converged!\")\n",
        "        else:\n",
        "            print(\"⚠ Model still learning/exploring\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_micro_learning()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
