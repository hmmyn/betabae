{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BetaBae Text Generation: Bhagavad Gita Analysis\n",
        "\n",
        "This notebook explores the text generation results from BetaBae trained on the Bhagavad Gita dataset.\n",
        "\n",
        "## What We're Observing:\n",
        "- **Attention Patterns**: How does the model learn to attend to relevant parts of the text?\n",
        "- **Language Learning**: Can we see the model learning grammar, structure, and meaning?\n",
        "- **Representation Evolution**: How do embeddings organize to capture linguistic structure?\n",
        "- **Emergence of Understanding**: When does the model start generating coherent text?\n",
        "\n",
        "## Key Questions:\n",
        "1. Does attention focus on semantically related words?\n",
        "2. Can we see the model learning sentence structure?\n",
        "3. How do representations cluster around different concepts?\n",
        "4. What patterns emerge in the generated text over time?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import re\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load dataset info\n",
        "from betabae.text_logger import GitaDataset\n",
        "\n",
        "dataset_path = '/home/abhaydjoshi/.cache/kagglehub/datasets/madhurpant/bhagavad-gita-verses-dataset/versions/1/bhagavad_gita_verses.csv'\n",
        "dataset = GitaDataset(dataset_path, seq_len=64, vocab_size=128)\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Text length: {len(dataset.text):,} characters\")\n",
        "print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
        "print(f\"Number of sequences: {len(dataset.tokens):,}\")\n",
        "print(f\"Sequence length: 64 tokens\")\n",
        "print(f\"\\nSample vocabulary: {dataset.vocab[:30]}\")\n",
        "print(f\"\\nSample text: {dataset.text[:300]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if training has produced any logs yet\n",
        "log_dir = Path('text_outputs/logs')\n",
        "if log_dir.exists():\n",
        "    epoch_files = sorted(log_dir.glob('epoch_*.npz'))\n",
        "    generation_files = sorted(log_dir.glob('generations_epoch_*.txt'))\n",
        "    \n",
        "    print(f\"Found {len(epoch_files)} epoch files\")\n",
        "    print(f\"Found {len(generation_files)} generation files\")\n",
        "    \n",
        "    if epoch_files:\n",
        "        print(f\"Epoch files: {[f.name for f in epoch_files[:5]]}\")\n",
        "        \n",
        "        # Load first epoch to check data structure\n",
        "        data = np.load(epoch_files[0])\n",
        "        print(f\"\\nData structure:\")\n",
        "        for key in data.keys():\n",
        "            print(f\"  {key}: {data[key].shape}\")\n",
        "    \n",
        "    if generation_files:\n",
        "        print(f\"\\nGeneration files: {[f.name for f in generation_files[:3]]}\")\n",
        "        \n",
        "        # Show sample generations\n",
        "        for gen_file in generation_files[:2]:\n",
        "            print(f\"\\n{gen_file.name}:\")\n",
        "            with open(gen_file, 'r') as f:\n",
        "                content = f.read()\n",
        "                print(content[:200] + \"...\")\n",
        "else:\n",
        "    print(\"Training logs not found yet. Training may still be in progress...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to analyze text generation results\n",
        "def analyze_text_results(log_dir):\n",
        "    \"\"\"Analyze text generation training results\"\"\"\n",
        "    \n",
        "    epoch_files = sorted(log_dir.glob('epoch_*.npz'))\n",
        "    generation_files = sorted(log_dir.glob('generations_epoch_*.txt'))\n",
        "    \n",
        "    if not epoch_files:\n",
        "        print(\"No training data found yet!\")\n",
        "        return\n",
        "    \n",
        "    # Collect all metrics\n",
        "    all_losses = []\n",
        "    all_perplexities = []\n",
        "    epoch_numbers = []\n",
        "    \n",
        "    for epoch_file in epoch_files:\n",
        "        data = np.load(epoch_file)\n",
        "        all_losses.extend(data['losses'])\n",
        "        \n",
        "        if 'perplexities' in data:\n",
        "            all_perplexities.extend(data['perplexities'])\n",
        "        else:\n",
        "            # Calculate perplexity from loss\n",
        "            all_perplexities.extend([np.exp(loss) for loss in data['losses']])\n",
        "        \n",
        "        epoch_num = int(epoch_file.stem.split('_')[1])\n",
        "        epoch_numbers.extend([epoch_num] * len(data['losses']))\n",
        "    \n",
        "    # Plot learning curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss over time\n",
        "    axes[0, 0].plot(all_losses, alpha=0.7, linewidth=0.5)\n",
        "    axes[0, 0].set_title('Training Loss Evolution')\n",
        "    axes[0, 0].set_xlabel('Training Step')\n",
        "    axes[0, 0].set_ylabel('Cross-Entropy Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Perplexity over time\n",
        "    axes[0, 1].plot(all_perplexities, alpha=0.7, linewidth=0.5, color='orange')\n",
        "    axes[0, 1].set_title('Perplexity Evolution')\n",
        "    axes[0, 1].set_xlabel('Training Step')\n",
        "    axes[0, 1].set_ylabel('Perplexity')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Loss by epoch\n",
        "    epoch_losses = []\n",
        "    for epoch_num in sorted(set(epoch_numbers)):\n",
        "        epoch_losses.append(np.mean([loss for i, loss in enumerate(all_losses) if epoch_numbers[i] == epoch_num]))\n",
        "    \n",
        "    axes[1, 0].plot(epoch_losses, marker='o', markersize=4)\n",
        "    axes[1, 0].set_title('Average Loss per Epoch')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Average Loss')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Perplexity by epoch\n",
        "    epoch_perplexities = []\n",
        "    for epoch_num in sorted(set(epoch_numbers)):\n",
        "        epoch_perplexities.append(np.mean([perp for i, perp in enumerate(all_perplexities) if epoch_numbers[i] == epoch_num]))\n",
        "    \n",
        "    axes[1, 1].plot(epoch_perplexities, marker='o', markersize=4, color='green')\n",
        "    axes[1, 1].set_title('Average Perplexity per Epoch')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Average Perplexity')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nTraining Summary:\")\n",
        "    print(f\"Total training steps: {len(all_losses)}\")\n",
        "    print(f\"Number of epochs: {len(set(epoch_numbers))}\")\n",
        "    print(f\"Initial loss: {all_losses[0]:.4f}\")\n",
        "    print(f\"Final loss: {all_losses[-1]:.4f}\")\n",
        "    print(f\"Initial perplexity: {all_perplexities[0]:.2f}\")\n",
        "    print(f\"Final perplexity: {all_perplexities[-1]:.2f}\")\n",
        "    \n",
        "    return all_losses, all_perplexities, epoch_numbers\n",
        "\n",
        "# Run analysis if data is available\n",
        "if log_dir.exists():\n",
        "    analyze_text_results(log_dir)\n",
        "else:\n",
        "    print(\"Training data not available yet. Run this cell after training produces logs.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
